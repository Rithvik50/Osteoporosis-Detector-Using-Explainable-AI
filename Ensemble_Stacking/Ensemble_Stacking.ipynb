{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Stacking"
      ],
      "metadata": {
        "id": "HXj8IqH6a18O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E3RSlbTQawsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0fbb5a-a0c5-40bd-81f4-c92415e617c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU scikit-learn xgboost lightgbm catboost optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.pruners import MedianPruner\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, PolynomialFeatures, OrdinalEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "gli4OXQo3usk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"final_synthesized_dataset.csv\")"
      ],
      "metadata": {
        "id": "5SFF7cL8MGlk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "g5t4RTj7isdG",
        "outputId": "58966faf-3bea-4efa-976b-e2e2af3db747"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age sex       ethnicity  height_cm  weight_kg   bmi  waist_cm  hip_cm  \\\n",
              "0   58   F           White      156.7       76.9  31.3     100.3   105.2   \n",
              "1   54   F  Middle_Eastern      162.9       74.5  28.1      85.2    86.5   \n",
              "2   47   F           White      149.0       84.1  37.9     112.2   128.6   \n",
              "3   76   M           White      174.4      107.2  35.3     122.3   132.3   \n",
              "4   56   M           White      171.2       68.7  23.4     103.1   113.8   \n",
              "\n",
              "   waist_hip_ratio  menopause_age  ... serum_calcium_mgdl  \\\n",
              "0            0.954             48  ...               9.39   \n",
              "1            0.985             46  ...               9.24   \n",
              "2            0.873              0  ...               9.34   \n",
              "3            0.924              0  ...               9.78   \n",
              "4            0.906              0  ...               9.20   \n",
              "\n",
              "   alkaline_phosphatase  pth_pgml  creatinine_mgdl  hdl_mgdl  ldl_mgdl  \\\n",
              "0                  45.7      60.2             0.96      69.0      99.1   \n",
              "1                 108.7      44.9             1.69      95.0     114.4   \n",
              "2                  81.4      53.7             1.18      60.2     171.0   \n",
              "3                  59.9      30.4             1.20      50.3     176.8   \n",
              "4                  64.0      68.0             1.16      61.3     150.5   \n",
              "\n",
              "   ctx_ngml  p1np_ugL         label  vitamin_d_missing  \n",
              "0     0.352      59.2    Osteopenia                  0  \n",
              "1     0.425      38.9  Osteoporosis                  1  \n",
              "2     0.220      42.0        Normal                  0  \n",
              "3     0.188      37.0        Normal                  0  \n",
              "4     0.339      41.0    Osteopenia                  0  \n",
              "\n",
              "[5 rows x 39 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92457773-3a0d-48c3-8cc9-f54deb58638c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>ethnicity</th>\n",
              "      <th>height_cm</th>\n",
              "      <th>weight_kg</th>\n",
              "      <th>bmi</th>\n",
              "      <th>waist_cm</th>\n",
              "      <th>hip_cm</th>\n",
              "      <th>waist_hip_ratio</th>\n",
              "      <th>menopause_age</th>\n",
              "      <th>...</th>\n",
              "      <th>serum_calcium_mgdl</th>\n",
              "      <th>alkaline_phosphatase</th>\n",
              "      <th>pth_pgml</th>\n",
              "      <th>creatinine_mgdl</th>\n",
              "      <th>hdl_mgdl</th>\n",
              "      <th>ldl_mgdl</th>\n",
              "      <th>ctx_ngml</th>\n",
              "      <th>p1np_ugL</th>\n",
              "      <th>label</th>\n",
              "      <th>vitamin_d_missing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>F</td>\n",
              "      <td>White</td>\n",
              "      <td>156.7</td>\n",
              "      <td>76.9</td>\n",
              "      <td>31.3</td>\n",
              "      <td>100.3</td>\n",
              "      <td>105.2</td>\n",
              "      <td>0.954</td>\n",
              "      <td>48</td>\n",
              "      <td>...</td>\n",
              "      <td>9.39</td>\n",
              "      <td>45.7</td>\n",
              "      <td>60.2</td>\n",
              "      <td>0.96</td>\n",
              "      <td>69.0</td>\n",
              "      <td>99.1</td>\n",
              "      <td>0.352</td>\n",
              "      <td>59.2</td>\n",
              "      <td>Osteopenia</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>54</td>\n",
              "      <td>F</td>\n",
              "      <td>Middle_Eastern</td>\n",
              "      <td>162.9</td>\n",
              "      <td>74.5</td>\n",
              "      <td>28.1</td>\n",
              "      <td>85.2</td>\n",
              "      <td>86.5</td>\n",
              "      <td>0.985</td>\n",
              "      <td>46</td>\n",
              "      <td>...</td>\n",
              "      <td>9.24</td>\n",
              "      <td>108.7</td>\n",
              "      <td>44.9</td>\n",
              "      <td>1.69</td>\n",
              "      <td>95.0</td>\n",
              "      <td>114.4</td>\n",
              "      <td>0.425</td>\n",
              "      <td>38.9</td>\n",
              "      <td>Osteoporosis</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>47</td>\n",
              "      <td>F</td>\n",
              "      <td>White</td>\n",
              "      <td>149.0</td>\n",
              "      <td>84.1</td>\n",
              "      <td>37.9</td>\n",
              "      <td>112.2</td>\n",
              "      <td>128.6</td>\n",
              "      <td>0.873</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.34</td>\n",
              "      <td>81.4</td>\n",
              "      <td>53.7</td>\n",
              "      <td>1.18</td>\n",
              "      <td>60.2</td>\n",
              "      <td>171.0</td>\n",
              "      <td>0.220</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>76</td>\n",
              "      <td>M</td>\n",
              "      <td>White</td>\n",
              "      <td>174.4</td>\n",
              "      <td>107.2</td>\n",
              "      <td>35.3</td>\n",
              "      <td>122.3</td>\n",
              "      <td>132.3</td>\n",
              "      <td>0.924</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.78</td>\n",
              "      <td>59.9</td>\n",
              "      <td>30.4</td>\n",
              "      <td>1.20</td>\n",
              "      <td>50.3</td>\n",
              "      <td>176.8</td>\n",
              "      <td>0.188</td>\n",
              "      <td>37.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56</td>\n",
              "      <td>M</td>\n",
              "      <td>White</td>\n",
              "      <td>171.2</td>\n",
              "      <td>68.7</td>\n",
              "      <td>23.4</td>\n",
              "      <td>103.1</td>\n",
              "      <td>113.8</td>\n",
              "      <td>0.906</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.20</td>\n",
              "      <td>64.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>61.3</td>\n",
              "      <td>150.5</td>\n",
              "      <td>0.339</td>\n",
              "      <td>41.0</td>\n",
              "      <td>Osteopenia</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 39 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92457773-3a0d-48c3-8cc9-f54deb58638c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-92457773-3a0d-48c3-8cc9-f54deb58638c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-92457773-3a0d-48c3-8cc9-f54deb58638c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-70de73ba-f1f1-435f-83bb-4fb338474305\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-70de73ba-f1f1-435f-83bb-4fb338474305')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-70de73ba-f1f1-435f-83bb-4fb338474305 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TXPEjsZJslC",
        "outputId": "a61d1c2e-de69-48c9-8f41-cd0267302ecb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9979 entries, 0 to 9978\n",
            "Data columns (total 39 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   age                     9979 non-null   int64  \n",
            " 1   sex                     9979 non-null   object \n",
            " 2   ethnicity               9979 non-null   object \n",
            " 3   height_cm               9979 non-null   float64\n",
            " 4   weight_kg               9979 non-null   float64\n",
            " 5   bmi                     9979 non-null   float64\n",
            " 6   waist_cm                9979 non-null   float64\n",
            " 7   hip_cm                  9979 non-null   float64\n",
            " 8   waist_hip_ratio         9979 non-null   float64\n",
            " 9   menopause_age           9979 non-null   int64  \n",
            " 10  menopausal_status       9979 non-null   object \n",
            " 11  years_since_menopause   9979 non-null   int64  \n",
            " 12  estrogen_use            9979 non-null   int64  \n",
            " 13  diabetes_t2             9979 non-null   int64  \n",
            " 14  hypothyroidism          9979 non-null   int64  \n",
            " 15  dialysis                9979 non-null   int64  \n",
            " 16  bisphosphonate_use      9979 non-null   int64  \n",
            " 17  prior_fracture          9979 non-null   int64  \n",
            " 18  parent_hip_fracture     9979 non-null   int64  \n",
            " 19  smoker                  9979 non-null   int64  \n",
            " 20  alcohol_high            9979 non-null   int64  \n",
            " 21  glucocorticoid_use      9979 non-null   int64  \n",
            " 22  rheumatoid_arthritis    9979 non-null   int64  \n",
            " 23  secondary_osteoporosis  9979 non-null   int64  \n",
            " 24  physical_activity       9979 non-null   object \n",
            " 25  calcium_supplement      9979 non-null   int64  \n",
            " 26  vitamin_d_supplement    9979 non-null   int64  \n",
            " 27  falls_past_year         9979 non-null   int64  \n",
            " 28  vitamin_d_ngml          9979 non-null   float64\n",
            " 29  serum_calcium_mgdl      9979 non-null   float64\n",
            " 30  alkaline_phosphatase    9979 non-null   float64\n",
            " 31  pth_pgml                9979 non-null   float64\n",
            " 32  creatinine_mgdl         9979 non-null   float64\n",
            " 33  hdl_mgdl                9979 non-null   float64\n",
            " 34  ldl_mgdl                9979 non-null   float64\n",
            " 35  ctx_ngml                9979 non-null   float64\n",
            " 36  p1np_ugL                9979 non-null   float64\n",
            " 37  label                   9979 non-null   object \n",
            " 38  vitamin_d_missing       9979 non-null   int64  \n",
            "dtypes: float64(15), int64(19), object(5)\n",
            "memory usage: 3.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation"
      ],
      "metadata": {
        "id": "b85INxVWtrIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StackingEnsembleOptuna:\n",
        "\n",
        "    def __init__(self, cv_folds=5, random_state=42, n_classes=3):\n",
        "        self.cv_folds = cv_folds\n",
        "        self.random_state = random_state\n",
        "        self.n_classes = n_classes\n",
        "        self.best_base_models = {}\n",
        "        self.best_meta_model = None\n",
        "        self.trained_pipelines = {}\n",
        "        self.study = None\n",
        "\n",
        "        # Define column types based on dataset info\n",
        "        self.numerical_cols = ['age', 'height_cm', 'weight_kg', 'bmi', 'waist_cm', 'hip_cm',\n",
        "                              'waist_hip_ratio', 'menopause_age', 'years_since_menopause',\n",
        "                              'vitamin_d_ngml', 'serum_calcium_mgdl', 'alkaline_phosphatase',\n",
        "                              'pth_pgml', 'creatinine_mgdl', 'hdl_mgdl', 'ldl_mgdl', 'ctx_ngml', 'p1np_ugL']\n",
        "\n",
        "        self.binary_cols = ['estrogen_use', 'diabetes_t2', 'hypothyroidism', 'dialysis',\n",
        "                           'bisphosphonate_use', 'prior_fracture', 'parent_hip_fracture',\n",
        "                           'smoker', 'alcohol_high', 'glucocorticoid_use', 'rheumatoid_arthritis',\n",
        "                           'secondary_osteoporosis', 'calcium_supplement', 'vitamin_d_supplement',\n",
        "                           'vitamin_d_missing']\n",
        "\n",
        "        self.categorical_cols = ['sex', 'ethnicity', 'menopausal_status', 'physical_activity']\n",
        "\n",
        "    def _get_preprocessor_for_model(self, model_name):\n",
        "        \"\"\"Create model-specific preprocessor based on model requirements\"\"\"\n",
        "\n",
        "        # Tree-based models\n",
        "        if model_name in ['decision_tree', 'random_forest', 'extra_trees', 'xgboost', 'lightgbm']:\n",
        "            return ColumnTransformer([\n",
        "                (\"num\", \"passthrough\", self.numerical_cols),\n",
        "                (\"bin\", \"passthrough\", self.binary_cols),\n",
        "                (\"cat\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), self.categorical_cols)  # Note: This needs special handling for multiple columns\n",
        "            ], remainder='drop')\n",
        "\n",
        "        # CatBoost can handle categorical natively\n",
        "        elif model_name == 'catboost':\n",
        "            return ColumnTransformer([\n",
        "                (\"num\", StandardScaler(), self.numerical_cols),\n",
        "                (\"bin\", \"passthrough\", self.binary_cols),\n",
        "                (\"cat\", \"passthrough\", self.categorical_cols)\n",
        "            ], remainder='drop')\n",
        "\n",
        "        # Linear models and others: StandardScaler + OneHotEncoder needed\n",
        "        else:  # logistic_regression, svm, polynomial, naive_bayes, knn, adaboost\n",
        "            return ColumnTransformer([\n",
        "                (\"num\", StandardScaler(), self.numerical_cols),\n",
        "                (\"bin\", StandardScaler(), self.binary_cols),  # Scale binary for linear models\n",
        "                (\"cat\", OneHotEncoder(drop='first', sparse_output=False), self.categorical_cols)\n",
        "            ], remainder='drop')\n",
        "\n",
        "    def _create_pipeline_for_model(self, model, model_name):\n",
        "        \"\"\"Create a pipeline with model-specific preprocessing\"\"\"\n",
        "\n",
        "        # Special handling for tree-based models that need label encoding\n",
        "        if model_name in ['decision_tree', 'random_forest', 'extra_trees', 'xgboost', 'lightgbm']:\n",
        "            # For tree-based models, we'll use a custom transformer for categorical encoding\n",
        "            class MultiLabelEncoder(BaseEstimator, TransformerMixin):\n",
        "                def __init__(self):\n",
        "                    self.label_encoders = {}\n",
        "\n",
        "                def fit(self, X, y=None):\n",
        "                    for col in X.columns:\n",
        "                        le = LabelEncoder()\n",
        "                        le.fit(X[col].astype(str))\n",
        "                        self.label_encoders[col] = le\n",
        "                    return self\n",
        "\n",
        "                def transform(self, X):\n",
        "                    X_encoded = X.copy()\n",
        "                    for col in X.columns:\n",
        "                        if col in self.label_encoders:\n",
        "                            X_encoded[col] = self.label_encoders[col].transform(X[col].astype(str))\n",
        "                    return X_encoded.values\n",
        "\n",
        "            preprocessor = ColumnTransformer([\n",
        "                (\"num\", \"passthrough\", self.numerical_cols),\n",
        "                (\"bin\", \"passthrough\", self.binary_cols),\n",
        "                (\"cat\", MultiLabelEncoder(), self.categorical_cols)\n",
        "            ], remainder='drop')\n",
        "        else:\n",
        "            preprocessor = self._get_preprocessor_for_model(model_name)\n",
        "\n",
        "        if model_name == \"polynomial\":\n",
        "            return Pipeline([\n",
        "                (\"preprocessor\", preprocessor),\n",
        "                (\"model\", model)\n",
        "            ])\n",
        "        else:\n",
        "            return Pipeline([\n",
        "                (\"preprocessor\", preprocessor),\n",
        "                (\"classifier\", model)\n",
        "            ])\n",
        "\n",
        "    def _create_base_model_search_space(self, trial, model_name):\n",
        "        \"\"\"Define hyperparameter search spaces for each base model\"\"\"\n",
        "\n",
        "        if model_name == \"logistic_regression\":\n",
        "            return LogisticRegression(\n",
        "                C=trial.suggest_float('lr_C', 1e-4, 1e2, log=True),\n",
        "                penalty=trial.suggest_categorical('lr_penalty', ['l1', 'l2']),\n",
        "                solver='liblinear' if trial.params.get('lr_penalty') in ['l1', 'l2'] else 'lbfgs',\n",
        "                max_iter=1000,\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "\n",
        "        elif model_name == \"svm\":\n",
        "            return SVC(\n",
        "                C=trial.suggest_float('svm_C', 1e-3, 1e3, log=True),\n",
        "                kernel=trial.suggest_categorical('svm_kernel', ['rbf', 'poly', 'sigmoid']),\n",
        "                gamma=trial.suggest_categorical('svm_gamma', ['scale', 'auto']) if trial.params.get('svm_kernel') != 'linear' else 'scale',\n",
        "                probability=True,\n",
        "                decision_function_shape='ovr',\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "\n",
        "        elif model_name == \"polynomial\":\n",
        "            degree = trial.suggest_int('poly_degree', 2, 4)\n",
        "            C = trial.suggest_float('poly_C', 1e-4, 1e2, log=True)\n",
        "            return Pipeline([\n",
        "                ('poly', PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "                ('logistic', LogisticRegression(\n",
        "                    C=C,\n",
        "                    max_iter=1000,\n",
        "                    multi_class='ovr',\n",
        "                    random_state=self.random_state\n",
        "                ))\n",
        "            ])\n",
        "\n",
        "        elif model_name == \"naive_bayes\":\n",
        "            return GaussianNB(\n",
        "                var_smoothing=trial.suggest_float('nb_var_smoothing', 1e-11, 1e-5, log=True)\n",
        "            )\n",
        "\n",
        "        elif model_name == \"knn\":\n",
        "            return KNeighborsClassifier(\n",
        "                n_neighbors=trial.suggest_int('knn_n_neighbors', 3, 20),\n",
        "                weights=trial.suggest_categorical('knn_weights', ['uniform', 'distance']),\n",
        "                metric=trial.suggest_categorical('knn_metric', ['euclidean', 'manhattan', 'minkowski'])\n",
        "            )\n",
        "\n",
        "        elif model_name == \"decision_tree\":\n",
        "            return DecisionTreeClassifier(\n",
        "                max_depth=trial.suggest_int('dt_max_depth', 3, 20),\n",
        "                min_samples_split=trial.suggest_int('dt_min_samples_split', 2, 20),\n",
        "                min_samples_leaf=trial.suggest_int('dt_min_samples_leaf', 1, 20),\n",
        "                max_features=trial.suggest_categorical('dt_max_features', ['sqrt', 'log2', None]),\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "\n",
        "        elif model_name == \"random_forest\":\n",
        "            return RandomForestClassifier(\n",
        "                n_estimators=trial.suggest_int('rf_n_estimators', 50, 300),\n",
        "                max_depth=trial.suggest_int('rf_max_depth', 3, 20),\n",
        "                min_samples_split=trial.suggest_int('rf_min_samples_split', 2, 20),\n",
        "                min_samples_leaf=trial.suggest_int('rf_min_samples_leaf', 1, 10),\n",
        "                max_features=trial.suggest_categorical('rf_max_features', ['sqrt', 'log2']),\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "\n",
        "        elif model_name == \"adaboost\":\n",
        "            return AdaBoostClassifier(\n",
        "                n_estimators=trial.suggest_int('ada_n_estimators', 50, 200),\n",
        "                learning_rate=trial.suggest_float('ada_learning_rate', 0.01, 2.0, log=True),\n",
        "                algorithm='SAMME',\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "\n",
        "        elif model_name == \"extra_trees\":\n",
        "            return ExtraTreesClassifier(\n",
        "                n_estimators=trial.suggest_int('et_n_estimators', 50, 300),\n",
        "                max_depth=trial.suggest_int('et_max_depth', 3, 20),\n",
        "                min_samples_split=trial.suggest_int('et_min_samples_split', 2, 20),\n",
        "                min_samples_leaf=trial.suggest_int('et_min_samples_leaf', 1, 10),\n",
        "                max_features=trial.suggest_categorical('et_max_features', ['sqrt', 'log2']),\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "\n",
        "        elif model_name == \"xgboost\":\n",
        "            return XGBClassifier(\n",
        "                n_estimators=trial.suggest_int('xgb_n_estimators', 50, 300),\n",
        "                max_depth=trial.suggest_int('xgb_max_depth', 3, 10),\n",
        "                learning_rate=trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True),\n",
        "                subsample=trial.suggest_float('xgb_subsample', 0.5, 1.0),\n",
        "                colsample_bytree=trial.suggest_float('xgb_colsample_bytree', 0.5, 1.0),\n",
        "                reg_alpha=trial.suggest_float('xgb_reg_alpha', 1e-8, 1.0, log=True),\n",
        "                reg_lambda=trial.suggest_float('xgb_reg_lambda', 1e-8, 1.0, log=True),\n",
        "                objective='multi:softprob',\n",
        "                num_class=self.n_classes,\n",
        "                random_state=self.random_state,\n",
        "                eval_metric='mlogloss',\n",
        "                enable_categorical=True\n",
        "            )\n",
        "\n",
        "        elif model_name == \"lightgbm\":\n",
        "            return LGBMClassifier(\n",
        "                n_estimators=trial.suggest_int('lgb_n_estimators', 50, 300),\n",
        "                max_depth=trial.suggest_int('lgb_max_depth', 3, 10),\n",
        "                learning_rate=trial.suggest_float('lgb_learning_rate', 0.01, 0.3, log=True),\n",
        "                subsample=trial.suggest_float('lgb_subsample', 0.5, 1.0),\n",
        "                colsample_bytree=trial.suggest_float('lgb_colsample_bytree', 0.5, 1.0),\n",
        "                reg_alpha=trial.suggest_float('lgb_reg_alpha', 1e-8, 1.0, log=True),\n",
        "                reg_lambda=trial.suggest_float('lgb_reg_lambda', 1e-8, 1.0, log=True),\n",
        "                objective='multiclass',\n",
        "                num_class=self.n_classes,\n",
        "                random_state=self.random_state,\n",
        "                verbose=-1,\n",
        "            )\n",
        "\n",
        "        elif model_name == \"catboost\":\n",
        "            return CatBoostClassifier(\n",
        "                iterations=trial.suggest_int('cat_iterations', 50, 300),\n",
        "                depth=trial.suggest_int('cat_depth', 3, 10),\n",
        "                learning_rate=trial.suggest_float('cat_learning_rate', 0.01, 0.3, log=True),\n",
        "                l2_leaf_reg=trial.suggest_float('cat_l2_leaf_reg', 1e-8, 10.0, log=True),\n",
        "                loss_function='MultiClass',\n",
        "                cat_features=list(range(len(self.numerical_cols) + len(self.binary_cols),\n",
        "                                      len(self.numerical_cols) + len(self.binary_cols) + len(self.categorical_cols))),\n",
        "                random_state=self.random_state,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "    def _create_meta_model_search_space(self, trial):\n",
        "        \"\"\"Define hyperparameter search space for meta-classifier\"\"\"\n",
        "\n",
        "        meta_model_type = trial.suggest_categorical('meta_model', ['logistic', 'random_forest', 'xgboost'])\n",
        "\n",
        "        if meta_model_type == 'logistic':\n",
        "            return LogisticRegression(\n",
        "                C=trial.suggest_float('meta_C', 1e-4, 1e2, log=True),\n",
        "                penalty=trial.suggest_categorical('meta_penalty', ['l1', 'l2']),\n",
        "                solver='liblinear',\n",
        "                multi_class='ovr',\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "        elif meta_model_type == 'random_forest':\n",
        "            return RandomForestClassifier(\n",
        "                n_estimators=trial.suggest_int('meta_rf_n_estimators', 50, 200),\n",
        "                max_depth=trial.suggest_int('meta_rf_max_depth', 3, 10),\n",
        "                random_state=self.random_state\n",
        "            )\n",
        "        else:\n",
        "            return XGBClassifier(\n",
        "                n_estimators=trial.suggest_int('meta_xgb_n_estimators', 50, 200),\n",
        "                max_depth=trial.suggest_int('meta_xgb_max_depth', 3, 6),\n",
        "                learning_rate=trial.suggest_float('meta_xgb_learning_rate', 0.01, 0.3),\n",
        "                objective='multi:softprob',\n",
        "                num_class=self.n_classes,\n",
        "                random_state=self.random_state,\n",
        "                eval_metric='mlogloss'\n",
        "            )\n",
        "\n",
        "    def _stacking_cross_validation(self, X, y, base_models, meta_model):\n",
        "        \"\"\"Perform stacking with cross-validation for multi-class\"\"\"\n",
        "\n",
        "        # Ensure y is numeric for roc_auc_score\n",
        "        if hasattr(y, 'dtype') and y.dtype == 'object':\n",
        "            temp_le = LabelEncoder()\n",
        "            y_numeric = temp_le.fit_transform(y)\n",
        "        elif isinstance(y, (list, tuple)) and isinstance(y[0], str):\n",
        "            temp_le = LabelEncoder()\n",
        "            y_numeric = temp_le.fit_transform(y)\n",
        "        else:\n",
        "            y_numeric = y\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
        "        n_samples = len(X)\n",
        "        n_models = len(base_models)\n",
        "\n",
        "        # OOF predictions matrix\n",
        "        oof_predictions = np.zeros((n_samples, n_models * self.n_classes))\n",
        "\n",
        "        # Generate OOF predictions for each base model\n",
        "        for model_idx, (model_name, model) in enumerate(base_models.items()):\n",
        "            for train_idx, val_idx in skf.split(X, y_numeric):\n",
        "                # Create pipeline for this model with model-specific preprocessing\n",
        "                pipeline = self._create_pipeline_for_model(clone(model), model_name)\n",
        "\n",
        "                # Split data\n",
        "                X_fold_train = X.iloc[train_idx] if hasattr(X, 'iloc') else X[train_idx]\n",
        "                X_fold_val = X.iloc[val_idx] if hasattr(X, 'iloc') else X[val_idx]\n",
        "\n",
        "                # Use numeric y for consistency\n",
        "                y_fold_train = y_numeric[train_idx] if hasattr(y_numeric, '__getitem__') else y_numeric.iloc[train_idx]\n",
        "                y_fold_val = y_numeric[val_idx] if hasattr(y_numeric, '__getitem__') else y_numeric.iloc[val_idx]\n",
        "\n",
        "                # Train and predict\n",
        "                pipeline.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "                # Get all class probabilities\n",
        "                val_probs = pipeline.predict_proba(X_fold_val)\n",
        "\n",
        "                # Store all class probabilities for this model\n",
        "                start_col = model_idx * self.n_classes\n",
        "                end_col = start_col + self.n_classes\n",
        "                oof_predictions[val_idx, start_col:end_col] = val_probs\n",
        "\n",
        "            # Train final pipeline on full data\n",
        "            final_pipeline = self._create_pipeline_for_model(clone(model), model_name)\n",
        "            final_pipeline.fit(X, y_numeric)\n",
        "            self.trained_pipelines[model_name] = final_pipeline\n",
        "\n",
        "        # Train meta-model\n",
        "        self.best_meta_model = clone(meta_model)\n",
        "        print(f\"Shape of OOF predictions for meta-model training: {oof_predictions.shape}\")\n",
        "        self.best_meta_model.fit(oof_predictions, y_numeric)\n",
        "\n",
        "        self.best_base_models = base_models\n",
        "\n",
        "        # Evaluate the stacking ensemble\n",
        "        meta_oof_predictions = self.best_meta_model.predict_proba(oof_predictions)\n",
        "\n",
        "        # Calculate multi-class ROC AUC using numeric labels\n",
        "        score = roc_auc_score(y_numeric, meta_oof_predictions, multi_class='ovr', average='macro')\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _objective(self, trial, X, y):\n",
        "        \"\"\"Optuna objective function\"\"\"\n",
        "\n",
        "        model_names = ['logistic_regression', 'svm', 'polynomial', 'naive_bayes', 'knn',\n",
        "                      'decision_tree', 'random_forest', 'adaboost', 'extra_trees',\n",
        "                      'xgboost', 'lightgbm', 'catboost']\n",
        "\n",
        "        base_models = {}\n",
        "        selected_model_names = []\n",
        "\n",
        "        # Suggest whether to include each model\n",
        "        for model_name in model_names:\n",
        "            include_model = trial.suggest_categorical(f'include_{model_name}', ['include', 'exclude'])\n",
        "            if include_model == 'include':\n",
        "                try:\n",
        "                    model = self._create_base_model_search_space(trial, model_name)\n",
        "                    base_models[model_name] = model\n",
        "                    selected_model_names.append(model_name)\n",
        "                except Exception as e:\n",
        "                    # Skip problematic model configurations\n",
        "                    continue\n",
        "\n",
        "        if len(base_models) < 2:\n",
        "            return 0.0  # Need at least 2 base models\n",
        "\n",
        "        # Store selected model names in trial params for later retrieval\n",
        "        trial.set_user_attr('selected_models', tuple(sorted(selected_model_names)))\n",
        "\n",
        "        # Create meta-model\n",
        "        meta_model = self._create_meta_model_search_space(trial)\n",
        "\n",
        "        try:\n",
        "            # Evaluate stacking ensemble and return the score\n",
        "            score = self._stacking_cross_validation(X, y, base_models, meta_model)\n",
        "            return score\n",
        "        except Exception as e:\n",
        "            print(f\"Trial failed: {e}\")\n",
        "            return 0.0  # Return poor score for failed configurations\n",
        "\n",
        "    def optimize(self, X, y, n_trials=100, timeout=None):\n",
        "        \"\"\"Optimize stacking ensemble using Optuna\"\"\"\n",
        "\n",
        "        print(f\"Starting Optuna optimization with {n_trials} trials---\")\n",
        "\n",
        "        self.study = optuna.create_study(\n",
        "            direction='maximize',\n",
        "            sampler=TPESampler(seed=self.random_state),\n",
        "            pruner=MedianPruner()\n",
        "        )\n",
        "\n",
        "        self.study.optimize(\n",
        "            lambda trial: self._objective(trial, X, y),\n",
        "            n_trials=n_trials,\n",
        "            timeout=timeout,\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "\n",
        "        print(f\"Optimization completed!\")\n",
        "        print(f\"Best score: {self.study.best_value:.4f}\")\n",
        "        print(f\"Best parameters: {self.study.best_params}\")\n",
        "\n",
        "        return self.study.best_value, self.study.best_params\n",
        "\n",
        "    def fit_best_model(self, X, y):\n",
        "        \"\"\"Fit the best model found by optimization\"\"\"\n",
        "        self.trained_pipelines = {}\n",
        "\n",
        "        if self.study is None:\n",
        "            raise ValueError(\"Must run optimize() first!\")\n",
        "\n",
        "        # Ensure y is numeric\n",
        "        if hasattr(y, 'dtype') and y.dtype == 'object':\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "            temp_le = LabelEncoder()\n",
        "            y_numeric = temp_le.fit_transform(y)\n",
        "        elif isinstance(y, (list, tuple)) and isinstance(y[0], str):\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "            temp_le = LabelEncoder()\n",
        "            y_numeric = temp_le.fit_transform(y)\n",
        "        else:\n",
        "            y_numeric = y\n",
        "\n",
        "        best_params = self.study.best_params\n",
        "\n",
        "        # Reconstruct best base models\n",
        "        base_models = {}\n",
        "\n",
        "        # Need a mock trial to reconstruct models with best params\n",
        "        class MockTrial:\n",
        "            def __init__(self, params):\n",
        "                self.params = params\n",
        "\n",
        "            def suggest_float(self, name, low, high, log=False):\n",
        "                return self.params.get(name, (low + high) / 2)\n",
        "\n",
        "            def suggest_int(self, name, low, high):\n",
        "                return self.params.get(name, (low + high) // 2)\n",
        "\n",
        "            def suggest_categorical(self, name, choices):\n",
        "                return self.params.get(name, choices[0])\n",
        "\n",
        "        mock_trial = MockTrial(best_params)\n",
        "        model_names = ['logistic_regression', 'svm', 'polynomial', 'naive_bayes', 'knn',\n",
        "                      'decision_tree', 'random_forest', 'adaboost', 'extra_trees',\n",
        "                      'xgboost', 'lightgbm', 'catboost']\n",
        "\n",
        "        for model_name in model_names:\n",
        "            if best_params.get(f'include_{model_name}') == 'include':\n",
        "                try:\n",
        "                    model = self._create_base_model_search_space(mock_trial, model_name)\n",
        "                    base_models[model_name] = model\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not reconstruct best base model {model_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Reconstruct best meta-model\n",
        "        meta_model = self._create_meta_model_search_space(mock_trial)\n",
        "\n",
        "        # Train final stacking model\n",
        "        skf = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)\n",
        "        n_samples = len(X)\n",
        "        n_models = len(base_models)\n",
        "\n",
        "        # Multi-class predictions\n",
        "        oof_predictions = np.zeros((n_samples, n_models * self.n_classes))\n",
        "\n",
        "        # Generate OOF predictions and train final models\n",
        "        for model_idx, (model_name, model) in enumerate(base_models.items()):\n",
        "            for train_idx, val_idx in skf.split(X, y_numeric):\n",
        "                # Create pipeline with model-specific preprocessing\n",
        "                pipeline = self._create_pipeline_for_model(clone(model), model_name)\n",
        "\n",
        "                # Split and train\n",
        "                X_fold_train = X.iloc[train_idx] if hasattr(X, 'iloc') else X[train_idx]\n",
        "                X_fold_val = X.iloc[val_idx] if hasattr(X, 'iloc') else X[val_idx]\n",
        "\n",
        "                # Use numeric y\n",
        "                y_fold_train = y_numeric[train_idx] if hasattr(y_numeric, '__getitem__') else y_numeric.iloc[train_idx]\n",
        "                y_fold_val = y_numeric[val_idx] if hasattr(y_numeric, '__getitem__') else y_numeric.iloc[val_idx]\n",
        "\n",
        "                pipeline.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "                # Get all class probabilities\n",
        "                val_probs = pipeline.predict_proba(X_fold_val)\n",
        "\n",
        "                # Store all class probabilities\n",
        "                start_col = model_idx * self.n_classes\n",
        "                end_col = start_col + self.n_classes\n",
        "                oof_predictions[val_idx, start_col:end_col] = val_probs\n",
        "\n",
        "            # Train final pipeline on full data\n",
        "            final_pipeline = self._create_pipeline_for_model(clone(model), model_name)\n",
        "            final_pipeline.fit(X, y_numeric)\n",
        "            self.trained_pipelines[model_name] = final_pipeline\n",
        "\n",
        "        # Train meta-model\n",
        "        self.best_meta_model = clone(meta_model)\n",
        "        self.best_meta_model.fit(oof_predictions, y_numeric)\n",
        "\n",
        "        self.best_base_models = base_models\n",
        "\n",
        "        print(f\"Best stacking model trained successfully!\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions using the trained stacking ensemble\"\"\"\n",
        "\n",
        "        if not self.trained_pipelines or self.best_meta_model is None:\n",
        "            raise ValueError(\"Must fit the model first!\")\n",
        "\n",
        "        n_samples = len(X)\n",
        "        n_models = len(self.trained_pipelines)\n",
        "\n",
        "        # Get base model predictions - all class probabilities\n",
        "        base_predictions = np.zeros((n_samples, n_models * self.n_classes))\n",
        "\n",
        "        for model_idx, (model_name, pipeline) in enumerate(self.trained_pipelines.items()):\n",
        "            base_probs = pipeline.predict_proba(X)\n",
        "\n",
        "            # Store all class probabilities\n",
        "            start_col = model_idx * self.n_classes\n",
        "            end_col = start_col + self.n_classes\n",
        "            base_predictions[:, start_col:end_col] = base_probs\n",
        "\n",
        "        # Meta-model final predictions\n",
        "        final_predictions = self.best_meta_model.predict(base_predictions)\n",
        "        return final_predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Get prediction probabilities\"\"\"\n",
        "\n",
        "        if not self.trained_pipelines or self.best_meta_model is None:\n",
        "            raise ValueError(\"Must fit the model first!\")\n",
        "\n",
        "        n_samples = len(X)\n",
        "        n_models = len(self.trained_pipelines)\n",
        "\n",
        "        # Get all class probabilities from base models\n",
        "        base_predictions = np.zeros((n_samples, n_models * self.n_classes))\n",
        "\n",
        "        for model_idx, (model_name, pipeline) in enumerate(self.trained_pipelines.items()):\n",
        "            base_probs = pipeline.predict_proba(X)\n",
        "\n",
        "            # Store all class probabilities\n",
        "            start_col = model_idx * self.n_classes\n",
        "            end_col = start_col + self.n_classes\n",
        "            base_predictions[:, start_col:end_col] = base_probs\n",
        "\n",
        "        print(f\"Shape of base probabilities for meta-model prediction_proba: {base_predictions.shape}\")\n",
        "        final_probabilities = self.best_meta_model.predict_proba(base_predictions)\n",
        "        return final_probabilities"
      ],
      "metadata": {
        "id": "e2yMdxe7sDX_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"label\", axis=1)\n",
        "y = df[\"label\"]"
      ],
      "metadata": {
        "id": "0bMcFGmr-E4f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "660eeb36"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCGKv1zg-sLQ",
        "outputId": "c6a313c1-bc57-479c-ec8e-a191f5fa1107"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 7983\n",
            "Test samples: 1996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233,
          "referenced_widgets": [
            "025160a73f4c4ba895681ebabdb1f0cb",
            "64d1f6c8d99a4a4bb4299436d80c033e",
            "4677c3ebb5f146e0a94d92e91569ef4e",
            "cade57ecdcee4fa79512d5c43914ac07",
            "8c96140b93b84dfdb019f7fd8153ae2f",
            "c065c5b69e1d4287b0d979003ddac689",
            "d5b8b20743204645b69e4789d09f2cfa",
            "06a64a2e2d5f44f0874cea296c2f6f28",
            "6a26e50124a2425a9fa9ae89b8584a37",
            "8763e0351fd54f66b3a83944866eae51",
            "cb75bf6b0b154826ac54db9dec7e39fd"
          ]
        },
        "id": "f7447afa",
        "outputId": "2c252cfe-e5c7-43ec-a920-d7c841a00a25"
      },
      "source": [
        "stacking_model = StackingEnsembleOptuna(\n",
        "        cv_folds=5,\n",
        "        random_state=42,\n",
        "        n_classes=3\n",
        ")\n",
        "\n",
        "best_score, best_params = stacking_model.optimize(\n",
        "        X_train, y_train,\n",
        "        n_trials=30,\n",
        "        timeout=600  # 10 minutes timeout\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-17 11:32:53,579] A new study created in memory with name: no-name-67186ab3-4307-4e8d-b038-87d28d41ee90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Optuna optimization with 30 trials---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "025160a73f4c4ba895681ebabdb1f0cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of OOF predictions for meta-model training: (7983, 21)\n",
            "[I 2025-09-17 11:36:23,877] Trial 0 finished with value: 0.9626076680599224 and parameters: {'include_logistic_regression': 'exclude', 'include_svm': 'include', 'svm_C': 0.008632008168602538, 'svm_kernel': 'sigmoid', 'svm_gamma': 'auto', 'include_polynomial': 'exclude', 'include_naive_bayes': 'include', 'nb_var_smoothing': 1.2329623163659818e-10, 'include_knn': 'exclude', 'include_decision_tree': 'include', 'dt_max_depth': 8, 'dt_min_samples_split': 13, 'dt_min_samples_leaf': 3, 'dt_max_features': None, 'include_random_forest': 'include', 'rf_n_estimators': 179, 'rf_max_depth': 13, 'rf_min_samples_split': 2, 'rf_min_samples_leaf': 7, 'rf_max_features': 'sqrt', 'include_adaboost': 'exclude', 'include_extra_trees': 'include', 'et_n_estimators': 74, 'et_max_depth': 15, 'et_min_samples_split': 10, 'et_min_samples_leaf': 2, 'et_max_features': 'sqrt', 'include_xgboost': 'include', 'xgb_n_estimators': 216, 'xgb_max_depth': 5, 'xgb_learning_rate': 0.05864129169696527, 'xgb_subsample': 0.7733551396716398, 'xgb_colsample_bytree': 0.5924272277627636, 'xgb_reg_alpha': 0.5710537951126793, 'xgb_reg_lambda': 0.01588775693167255, 'include_lightgbm': 'include', 'lgb_n_estimators': 200, 'lgb_max_depth': 10, 'lgb_learning_rate': 0.01351182947645082, 'lgb_subsample': 0.5979914312095727, 'lgb_colsample_bytree': 0.522613644455269, 'lgb_reg_alpha': 4.005370050283172e-06, 'lgb_reg_lambda': 1.2865252594826764e-05, 'include_catboost': 'exclude', 'meta_model': 'xgboost', 'meta_xgb_n_estimators': 71, 'meta_xgb_max_depth': 6, 'meta_xgb_learning_rate': 0.03161968666713354}. Best is trial 0 with value: 0.9626076680599224.\n",
            "Shape of OOF predictions for meta-model training: (7983, 15)\n",
            "[I 2025-09-17 11:45:17,509] Trial 1 finished with value: 0.9941860556878117 and parameters: {'include_logistic_regression': 'include', 'lr_C': 0.001557019634551662, 'lr_penalty': 'l2', 'include_svm': 'exclude', 'include_polynomial': 'include', 'poly_degree': 3, 'poly_C': 0.0004956947932799964, 'include_naive_bayes': 'include', 'nb_var_smoothing': 9.66914669401601e-10, 'include_knn': 'exclude', 'include_decision_tree': 'exclude', 'include_random_forest': 'exclude', 'include_adaboost': 'include', 'ada_n_estimators': 157, 'ada_learning_rate': 0.5631047841350824, 'include_extra_trees': 'exclude', 'include_xgboost': 'exclude', 'include_lightgbm': 'include', 'lgb_n_estimators': 77, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.08710745900492396, 'lgb_subsample': 0.6571779905381634, 'lgb_colsample_bytree': 0.7542853455823514, 'lgb_reg_alpha': 0.1821930437934653, 'lgb_reg_lambda': 9.8704700073667e-07, 'include_catboost': 'exclude', 'meta_model': 'xgboost', 'meta_xgb_n_estimators': 74, 'meta_xgb_max_depth': 6, 'meta_xgb_learning_rate': 0.2443549100736809}. Best is trial 1 with value: 0.9941860556878117.\n",
            "Optimization completed!\n",
            "Best score: 0.9942\n",
            "Best parameters: {'include_logistic_regression': 'include', 'lr_C': 0.001557019634551662, 'lr_penalty': 'l2', 'include_svm': 'exclude', 'include_polynomial': 'include', 'poly_degree': 3, 'poly_C': 0.0004956947932799964, 'include_naive_bayes': 'include', 'nb_var_smoothing': 9.66914669401601e-10, 'include_knn': 'exclude', 'include_decision_tree': 'exclude', 'include_random_forest': 'exclude', 'include_adaboost': 'include', 'ada_n_estimators': 157, 'ada_learning_rate': 0.5631047841350824, 'include_extra_trees': 'exclude', 'include_xgboost': 'exclude', 'include_lightgbm': 'include', 'lgb_n_estimators': 77, 'lgb_max_depth': 3, 'lgb_learning_rate': 0.08710745900492396, 'lgb_subsample': 0.6571779905381634, 'lgb_colsample_bytree': 0.7542853455823514, 'lgb_reg_alpha': 0.1821930437934653, 'lgb_reg_lambda': 9.8704700073667e-07, 'include_catboost': 'exclude', 'meta_model': 'xgboost', 'meta_xgb_n_estimators': 74, 'meta_xgb_max_depth': 6, 'meta_xgb_learning_rate': 0.2443549100736809}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nOptimization Results:\")\n",
        "print(f\"Best CV Score: {best_score:.4f}\")\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in best_params.items():\n",
        "  print(f\"  {param}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90ci2T14FUw",
        "outputId": "ca65459a-e091-4433-c743-0c62103c7e69"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization Results:\n",
            "Best CV Score: 0.9942\n",
            "\n",
            "Best Parameters:\n",
            "  include_logistic_regression: include\n",
            "  lr_C: 0.001557019634551662\n",
            "  lr_penalty: l2\n",
            "  include_svm: exclude\n",
            "  include_polynomial: include\n",
            "  poly_degree: 3\n",
            "  poly_C: 0.0004956947932799964\n",
            "  include_naive_bayes: include\n",
            "  nb_var_smoothing: 9.66914669401601e-10\n",
            "  include_knn: exclude\n",
            "  include_decision_tree: exclude\n",
            "  include_random_forest: exclude\n",
            "  include_adaboost: include\n",
            "  ada_n_estimators: 157\n",
            "  ada_learning_rate: 0.5631047841350824\n",
            "  include_extra_trees: exclude\n",
            "  include_xgboost: exclude\n",
            "  include_lightgbm: include\n",
            "  lgb_n_estimators: 77\n",
            "  lgb_max_depth: 3\n",
            "  lgb_learning_rate: 0.08710745900492396\n",
            "  lgb_subsample: 0.6571779905381634\n",
            "  lgb_colsample_bytree: 0.7542853455823514\n",
            "  lgb_reg_alpha: 0.1821930437934653\n",
            "  lgb_reg_lambda: 9.8704700073667e-07\n",
            "  include_catboost: exclude\n",
            "  meta_model: xgboost\n",
            "  meta_xgb_n_estimators: 74\n",
            "  meta_xgb_max_depth: 6\n",
            "  meta_xgb_learning_rate: 0.2443549100736809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_model.fit_best_model(X_train, y_train)"
      ],
      "metadata": {
        "id": "RxgKGa8J_7jG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c4a3a3-21d7-42b8-c224-057282a6ee01"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best stacking model trained successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.StackingEnsembleOptuna at 0x7a36f38d11f0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = stacking_model.predict(X_test)\n",
        "test_probabilities = stacking_model.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "q0MrcXs__WR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33c8952-9cc2-47dc-c013-4672fb15ccd2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of base probabilities for meta-model prediction_proba: (1996, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "\n",
        "le.fit(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test_encoded, test_predictions)\n",
        "auc = roc_auc_score(y_test_encoded, test_probabilities, multi_class='ovr', average='macro')\n",
        "\n",
        "print(f\"FINAL RESULTS\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Best CV Score: {best_score:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test AUC: {auc:.4f} \\n\")\n",
        "print(f\"Confusion Matrix: \\n {confusion_matrix(y_test_encoded, test_predictions)} \\n\")\n",
        "print(f\"Models used: {list(stacking_model.trained_pipelines.keys())}\")\n",
        "print(classification_report(y_test_encoded, test_predictions,\n",
        "                          target_names=['Normal', 'Osteopenia', 'Osteoporosis']))"
      ],
      "metadata": {
        "id": "fYdtD8ok_ySW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a9382a-a7fb-4918-90c3-ed53d6368623"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL RESULTS\n",
            "----------------------------------------\n",
            "Best CV Score: 0.9942\n",
            "Test Accuracy: 0.7816\n",
            "Test AUC: 0.9288 \n",
            "\n",
            "Confusion Matrix: \n",
            " [[511 142   0]\n",
            " [126 730  85]\n",
            " [  0  83 319]] \n",
            "\n",
            "Models used: ['logistic_regression', 'polynomial', 'naive_bayes', 'adaboost', 'lightgbm']\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.80      0.78      0.79       653\n",
            "  Osteopenia       0.76      0.78      0.77       941\n",
            "Osteoporosis       0.79      0.79      0.79       402\n",
            "\n",
            "    accuracy                           0.78      1996\n",
            "   macro avg       0.79      0.78      0.78      1996\n",
            "weighted avg       0.78      0.78      0.78      1996\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
